{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSobol/GNNCommunityDetection/blob/main/Embedding_NN_NetClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73c8k8DtR2w",
        "outputId": "2ad1219d-0df3-452f-b58c-454155646224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycombo in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: pybind11<3.0.0,>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pycombo) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from pycombo) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->pycombo) (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycombo\n",
        "#!pip install fastnode2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network community detection based on unsupervised node embedding clustering using Vanilla Neural Networks**\n",
        "\n",
        "* Learn node embedding based on random walk probability distributions with home teleport\n",
        "\n",
        "* Baseline community detection through embedding clustering\n",
        "\n",
        "* VNN node clustering based on node embedding aiming to optimize modularity\n",
        "\n",
        " ** Unsupervised VNN training from scratch fails\n",
        "\n",
        " ** Supervised VNN pre-tune to match clustering results helps\n",
        "\n",
        " ** Further unsupervised finetune optimizing modularity can reach close to best known partition scores (not reached by clustering), but performance is rather unstable"
      ],
      "metadata": {
        "id": "tOHcDuF0O9Ea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_w3g44x-G1IV"
      },
      "outputs": [],
      "source": [
        "import pycombo\n",
        "#import fastnode2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aH23YvKuTW9y"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.mixture import GaussianMixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fA63Vk9HG1IW"
      },
      "outputs": [],
      "source": [
        "#load other presaved networks\n",
        "def loadNetworkMat(filename, path = 'SampleNetworks/ProcessedMat/'):\n",
        "    A = scipy.io.loadmat(path + filename)\n",
        "    if check_symmetric(A['net']):\n",
        "        G = nx.from_numpy_matrix(A['net'])\n",
        "    else:\n",
        "        G = nx.from_numpy_matrix(A['net'], create_using=nx.DiGraph)\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def residential_page_rank_embedding(A, alpha=0.85): #get node embedding vector as probabilities of visiting other nodes with a network random walk and an 1-alpha percent home teleport home\n",
        "    w = A.sum(axis = 1).reshape(-1,1)\n",
        "    n = A.shape[0]\n",
        "    A = (A + (w == 0)) / (w + n * (w == 0))\n",
        "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
        "    X = (1 - alpha) * AI\n",
        "    return X"
      ],
      "metadata": {
        "id": "0FB853KKIrty"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S7bPMdVUTkPm"
      },
      "outputs": [],
      "source": [
        "#auxiliary routines (heritage - unused in this version)\n",
        "def modularity_matrix(adj): #get modularity matrix from adjacency matrix (heritage)\n",
        "    w_in = adj.sum(dim=0, keepdim=True)\n",
        "    w_out = adj.sum(dim=1, keepdim=True)\n",
        "    T = w_out.sum()\n",
        "    Q = adj / T - w_out * w_in / T ** 2\n",
        "    return Q\n",
        "\n",
        "def modularity(Q, partition): #get a modularity score for a partition (heritage)\n",
        "    return (Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()\n",
        "\n",
        "def node2vec_embedding(G: nx.Graph, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42): #node2vec emebdding to explore as an alternative\n",
        "    if nx.is_weighted(G):\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1]), edge[2]['weight']) for edge in G.edges(data=True)],\n",
        "                directed=False, weighted=True)\n",
        "    else:\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1])) for edge in G.edges(data=True)],\n",
        "                    directed=False, weighted=False)\n",
        "    n2v = fastnode2vec.Node2Vec(n2v_graph, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    n2v.train(epochs=100)\n",
        "    n2v_embeddings = np.array([n2v.wv[str(node)] for node in G])\n",
        "    return n2v_embeddings\n",
        "\n",
        "def rpr_clustering(A: np.array, n_clusters=4, kmeans_runs=100, alpha=0.85, seed=42): #cluster network based on our embedding (heritage)\n",
        "    rpr_embedding = residential_page_rank_embedding(A, alpha)\n",
        "    X = StandardScaler().fit_transform(X=rpr_embedding)\n",
        "    rpr_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(X).labels_\n",
        "    return rpr_cluster_labels\n",
        "\n",
        "def n2v_clustering(G: nx.Graph, n_clusters=4, kmeans_runs=100, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42): #cluster network based on node2vec embedding  (heritage)\n",
        "    n2v_embeddings = node2vec_embedding(G, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    X = StandardScaler().fit_transform(X=n2v_embeddings)\n",
        "    n2v_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(X).labels_\n",
        "    return n2v_cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W15lSK_eM0pN"
      },
      "outputs": [],
      "source": [
        "def aggregateEmbedding(X, n_groups): #produce aggregate embedding through node clustering - visiting one of the n_groups node clusters based on the embedding X, with a network random walk and an 1-alpha percent home teleport home\n",
        "  alpha = 0.85\n",
        "  kmeans_runs = 100\n",
        "  seed = 1\n",
        "  XS = StandardScaler().fit_transform(X = X)\n",
        "  model = KMeans(n_clusters = n_groups, n_init=kmeans_runs, random_state=seed).fit(XS)\n",
        "  #model = GaussianMixture(n_components = n_groups, n_init=kmeans_runs, random_state=seed).fit(X)\n",
        "  #rpr_cluster_labels = model.predict_proba(X)\n",
        "  cluster_labels = model.predict(XS)\n",
        "  XC = model.cluster_centers_\n",
        "  dist2 = np.array([[((XS[i, :] - XC[j, :])**2).sum() for j in range(n_groups)] for i in range(X.shape[0])])\n",
        "  sigma2 = dist2.min(axis = 1).mean()\n",
        "  probs = np.exp(- dist2 / sigma2)\n",
        "  probs /= probs.sum(axis = 1).reshape(-1,1)\n",
        "  XA = np.matmul(X, probs)\n",
        "  return XA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nDvODpJ0E8I7"
      },
      "outputs": [],
      "source": [
        "def modularity_matrix_np(adj): #numpy version of modularity matrix\n",
        "    if not(isinstance(adj, np.ndarray)):\n",
        "        adj = nx.to_numpy_array(G)\n",
        "    w_in = adj.sum(axis=0).reshape(-1,1)\n",
        "    w_out = adj.sum(axis=1).reshape(1,-1)\n",
        "    T = w_out.sum()\n",
        "    Q = adj / T - w_out * w_in / T ** 2\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IIIhwuZSG1IZ"
      },
      "outputs": [],
      "source": [
        "def clusterEmbeddings(X, n_comms = 2, cluststyle = 'k-means', seed = 1, runs = 100): #cluser nodes by provided embeddings\n",
        "    if cluststyle == 'GM':\n",
        "        XS = StandardScaler().fit_transform(X = X)\n",
        "        model = GaussianMixture(n_components = n_comms, n_init=runs, random_state=seed).fit(XS)\n",
        "        partition = model.predict(XS)\n",
        "    else:\n",
        "        XS = StandardScaler().fit_transform(X = X)\n",
        "        model = KMeans(n_clusters=n_comms, n_init=runs, random_state=seed).fit(XS)\n",
        "        partition = model.labels_\n",
        "    return partition, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yLjSfMb_G1IZ"
      },
      "outputs": [],
      "source": [
        "def produceAggEmbed(A, n_agg, alpha = 0.85): #produce and stack together hierarchical embeddings of a given resolutions (list n_agg, 0 stands for original network resulution)\n",
        "    X = residential_page_rank_embedding(A, alpha = alpha)\n",
        "    X_ = np.zeros((X.shape[0],0))\n",
        "    for na in n_agg:\n",
        "        if na == 0:\n",
        "            XA = X.copy()\n",
        "        else:\n",
        "            XA = aggregateEmbedding(X, n_groups = na)\n",
        "        X_ = np.concatenate((X_, XA), axis = 1)\n",
        "    return X_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0IH9HxBD8nj9"
      },
      "outputs": [],
      "source": [
        "#Gs =  [(nx.from_numpy_array(nx.to_numpy_array(nx.karate_club_graph(), weight=None)), 'karate', 4)]#, (nx.les_miserables_graph(),'lesmis',6)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qlMKCgAjG1Ia"
      },
      "outputs": [],
      "source": [
        "#processNet('karate_34') #karate_34 & 0.419790 & 0.419790 & 0.419790 & 0.419790 & 0.05 & 0.12 & 0.14 & 1.25\n",
        "#processNet('dolphins_62') #ok dolphins_62 & 0.527728 & 0.526463 & 0.527728 & 0.528519 & 0.11 & 0.15 & 0.10 & 1.13\n",
        "#processNet('football_115') #ok football_115 & 0.605445 & 0.605445 & 0.605445 & 0.605445 & 0.22 & 0.26 & 0.34 & 2.89\n",
        "#        processNet('polbooks_105') #ok polbooks_105 & 0.526967 & 0.527237 & 0.527237 & 0.527237 & 0.22 & 0.23 & 0.18 & 1.57\n",
        "#        processNet('copperfield_112') #ok ccopperfield_112 & 0.303028 & 0.309556 & 0.310173 & 0.313301 & 0.29 & 0.33 & 0.22 & 1.99\n",
        "#        processNet('email_1133')  # ok - undirected, unweighted email_1133 & 0.573062 & 0.582755 & 0.572484 & 0.578221 & 5.15 & 47.57 & 7.36 & 71.62\n",
        "#        processNet('lesmis_77') #ok lesmis_77 & 0.566688 & 0.566688 & 0.566688 & 0.566688 & 0.12 & 0.16 & 0.24 & 1.94\n",
        "#        processNet('celegansneural_297')  # directed celegansneural_297 & 0.503509 & 0.507642 & 0.505710 & 0.507642 & 16.43 & 1.19 & 1.46 & 8.61 | seed = 1: & 0.503509 & 0.507605 & 0.506235 & 0.507642 & 16.32 & 1.09 & 1.45 & 8.26\n",
        "#        processNet('jazz_198')  # celegansmetabolic_198 & 0.445627 & 0.444787 & 0.445522 & 0.445627 & 6.28 & 0.35 & 0.83 & 5.50\n",
        "#        processNet('USAir97_332')  # seed = 1; USAir97_332 & 0.207925 & 0.215244 & 0.217149 & 0.217751 & 20.27 & 1.08 & 1.72 & 9.29\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66GlS2NRN56_",
        "outputId": "3339b094-85e8-4253-dff0-a791d46b676d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network  lesmis_77 ; communities: 6\n"
          ]
        }
      ],
      "source": [
        "#load the selected network\n",
        "\n",
        "nname = 'lesmis_77' \n",
        "#nname = 'copperfield_112'\n",
        "#nname = 'polbooks_105'\n",
        "#nname = 'jazz_198' #0.443\n",
        "#G = loadNetworkMat(nname)\n",
        "#G = nx.karate_club_graph()\n",
        "G = nx.les_miserables_graph() #load les miserables\n",
        "n_comms = 6\n",
        "\n",
        "print(\"Network \", nname, '; communities:', n_comms)\n",
        "A = nx.to_numpy_array(G)\n",
        "Q = modularity_matrix_np(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-0g0ZlEAsad",
        "outputId": "0748e4f9-83d8-4216-cce4-8a6d710ab5e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5647873289708507"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "#produce and partition hierarchical embeddings\n",
        "#cluststyle = 'GM' \n",
        "cluststyle = 'k-means'\n",
        "seed = 12\n",
        "X_ = produceAggEmbed(A, n_agg = [15, 6], alpha = 0.85) #stacked hierachical embeddings of given resolutions\n",
        "N = X_.shape[1]\n",
        "partition, _ = clusterEmbeddings(X = X_, n_comms = n_comms, cluststyle = cluststyle, seed = seed)\n",
        "Q = modularity_matrix_np(A)\n",
        "(Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum() #return modularity of the resulting clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2Qx-huRVG1Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e5f63a-ec86-400f-9160-e23829a79f29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.566687983343249"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "combo_comms, combo_mod = pycombo.execute(G, max_communities = n_comms); combo_mod #COMBO modularity for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fwmyXMYvTcHf"
      },
      "outputs": [],
      "source": [
        "#Vanilla neural network\n",
        "class VNNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(VNNLayer, self).__init__()\n",
        "        self.weight1 = nn.Parameter(torch.randn(in_features, out_features)) # 0.5 * torch.eye(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(1, out_features)) # -0.5 * torch.ones(1, out_features))\n",
        "        self.dropout = dropout\n",
        "        #xavier initialization\n",
        "        torch.nn.init.xavier_uniform(self.weight1)\n",
        "        torch.nn.init.xavier_uniform(self.bias)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        v1 = torch.mm(input, self.weight1)\n",
        "        output = v1 + self.bias\n",
        "        output = F.dropout(output, p=self.dropout, training=self.training)\n",
        "        return output\n",
        "\n",
        "class VNN_MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, hid_layers = 0, hidden_dim=8, dropout=0.0, SEED = 1):\n",
        "        #super(VNN_MLP, self).__init__()\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        super().__init__()\n",
        "        self.n_layers = hid_layers + 1\n",
        "        self.hidden_dim = hidden_dim\n",
        "        if self.n_layers > 1:\n",
        "            layers = [VNNLayer(in_features, self.hidden_dim, dropout)]\n",
        "        else:\n",
        "            layers = [VNNLayer(in_features, out_features, dropout)]\n",
        "        for _ in range(self.n_layers-2):\n",
        "            layers.append(VNNLayer(self.hidden_dim, self.hidden_dim, dropout))\n",
        "        #if self.n_layers > 1:\n",
        "        layers.append(VNNLayer(self.hidden_dim, out_features, dropout = 0))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = self.layers[i](x)\n",
        "            x = nn.ReLU()(x)\n",
        "        x = self.layers[-1](x)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "        #x = 1.0 + x - x.max(dim=-1, keepdim=True).values\n",
        "        #x = torch.clamp(x, 0, 1)\n",
        "        #x = x / x.sum(dim=-1, keepdim=True) #normalize st sum = 1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C8s3aD4lG1Ib"
      },
      "outputs": [],
      "source": [
        "#network partition class based on VNN\n",
        "class VNNpartitioner (VNN_MLP):\n",
        "    def __init__(self, A, X, out_features = 2, hid_layers = 1, hidden_dim = 12, dropout = 0.0, SEED = 1):\n",
        "        self.adj = torch.FloatTensor(A)\n",
        "        self.Q = modularity_matrix(self.adj)\n",
        "        self.features = torch.FloatTensor(X)\n",
        "        super().__init__(self.features.shape[1], out_features = out_features, hid_layers = hid_layers, hidden_dim = hidden_dim, dropout = dropout, SEED = SEED)\n",
        "         \n",
        "    def fitSupervised(self, target_comms, n_epochs = 1000, lr = 0.005, SEED = 1, batchsize = 0): #supervised node classification based on given target labels\n",
        "        track_best = True\n",
        "        C0 = torch.tensor(to_categorical(target_comms))\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        t_total = time.time()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            batchind = range(self.features.shape[0])\n",
        "            if batchsize > 0:\n",
        "                batchind = np.random.choice(batchind, batchsize) #batching\n",
        "            t_1run = time.time()\n",
        "            optimizer.zero_grad()\n",
        "            out_embed = self.forward(self.features[batchind, :])\n",
        "            C = out_embed[:, :n_comms]\n",
        "            loss = torch.mean(torch.square(torch.subtract(C,C0[batchind,:]))) #torch.nn.functional.binary_cross_entropy(C, C0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if track_best: #store best states so far\n",
        "                if batchsize > 0:\n",
        "                    full_embed = self.forward(self.features)\n",
        "                    C = full_embed[:, :n_comms]\n",
        "                    full_loss = torch.mean(torch.square(torch.subtract(C,C0)))\n",
        "                else:\n",
        "                    full_loss = loss\n",
        "            \n",
        "                if epoch == 0 or full_loss < best_loss:\n",
        "                    best_loss = full_loss #- torch.trace(Q)\n",
        "                    best_C = C.data\n",
        "                    best_embed = out_embed.data\n",
        "                    best_epoch = epoch\n",
        "                \n",
        "            else:\n",
        "                full_loss = loss\n",
        "                best_loss = loss\n",
        "                \n",
        "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
        "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                print('Epoch: {:04d}'.format(epoch + 1), 'batch MSE: {:.8f}'.format(loss.item()), 'full MSE: {:.8f}'.format(full_loss.item()),\n",
        "                        'best MSE: {:.8f}'.format(best_loss.item()),\n",
        "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "        ent = best_loss.item()\n",
        "        print(\"Optimization Finished with MSE \", ent)\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "        return C, ent\n",
        "    \n",
        "    def fitUnsupervised(self, n_epochs = 1000, lr = 0.005, SEED = 1, batchsize = 0, restore_best = 500): #unsupervised node embedding clustering optimizing resulting modularity score\n",
        "        track_best = True\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        t_total = time.time()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        \n",
        "        out_embed = self.forward(self.features)\n",
        "        C = out_embed[:, :n_comms].data\n",
        "        \n",
        "        for epoch in range(n_epochs):\n",
        "            t_1run = time.time()\n",
        "            optimizer.zero_grad()\n",
        "            batchind = range(self.features.shape[0])\n",
        "            if batchsize > 0:\n",
        "                batchind = np.random.choice(batchind, batchsize)\n",
        "            out_embed = self.forward(self.features[batchind, :])\n",
        "            C[batchind, :] = out_embed[:, :n_comms]\n",
        "            Q1 = torch.mm(C.T, self.Q)\n",
        "            Q2 = torch.mm(Q1, C)\n",
        "            loss = torch.trace(Q2)\n",
        "            loss = -loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            C = C.data\n",
        "            \n",
        "            if track_best: #store best options so far\n",
        "                if batchsize > 0:\n",
        "                    full_embed = self.forward(self.features)\n",
        "                    C = full_embed[:, :n_comms].data\n",
        "                    full_loss = - torch.trace(torch.mm(torch.mm(C.T, self.Q), C))\n",
        "                else:\n",
        "                    full_loss = loss\n",
        "            \n",
        "                if epoch == 0 or full_loss < best_loss:\n",
        "                    best_loss = full_loss #- torch.trace(Q)\n",
        "                    best_C = C.data\n",
        "                    best_embed = out_embed.data\n",
        "                    best_epoch = epoch\n",
        "                    \n",
        "                    torch.save(self.state_dict(), 'model_scripted.pt')\n",
        "                    \n",
        "                if (epoch + 1) % restore_best == 0:\n",
        "                    self.load_state_dict(torch.load('model_scripted.pt'))\n",
        "                    self.eval()\n",
        "                \n",
        "            else:\n",
        "                full_loss = loss\n",
        "                best_loss = loss\n",
        "            \n",
        "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
        "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                print('Epoch: {:04d}'.format(epoch + 1), 'modularity: batch: {:.8f}'.format(-loss.item()), 'full: {:.8f}'.format(-full_loss.item()),\n",
        "                        'best: {:.8f}'.format(-best_loss.item()),\n",
        "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "                #print('Epoch: {:04d}'.format(epoch + 1),\n",
        "                #        'Modularity: {:.8f}'.format(-best_loss.item()),\n",
        "                #        'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "        mod = -best_loss.item()\n",
        "        print(\"Optimization Finished with modularity \", mod)\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "        return C, mod\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "lQ5fKcm3G1Id",
        "outputId": "d8c8d842-f7b7-422a-e144-17949a879216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "partitioner = VNNpartitioner(A, X_, out_features = n_comms, hid_layers = 4, hidden_dim = 2 * N, dropout = 0.0, SEED = 1) #init VNN partitioner - 4 hidden layers or input dim neurons each "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "partitioner.fitUnsupervised(n_epochs = 10000, lr = 0.0005, SEED = 1, batchsize = 20, restore_best = 200); #unsupervised node embedding clustering optimizing modularity - stuck early in local maxima if run from scratch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4UQQNdhQQbB",
        "outputId": "49c28cdd-2bf1-4575-f932-23037aa96330"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 modularity: batch: 0.00003167 full: 0.00003410 best: 0.00003410 time: 0.0079s\n",
            "Epoch: 0501 modularity: batch: 0.38129368 full: 0.38129500 best: 0.38129500 time: 0.0035s\n",
            "Epoch: 1001 modularity: batch: 0.38142294 full: 0.38142291 best: 0.38142294 time: 0.0025s\n",
            "Epoch: 1501 modularity: batch: 0.38143337 full: 0.38143337 best: 0.38143337 time: 0.0025s\n",
            "Epoch: 2001 modularity: batch: 0.38143638 full: 0.38143641 best: 0.38143641 time: 0.0036s\n",
            "Epoch: 2501 modularity: batch: 0.38143778 full: 0.38143781 best: 0.38143781 time: 0.0024s\n",
            "Epoch: 3001 modularity: batch: 0.38143849 full: 0.38143846 best: 0.38143849 time: 0.0024s\n",
            "Epoch: 3501 modularity: batch: 0.38143894 full: 0.38143894 best: 0.38143897 time: 0.0024s\n",
            "Epoch: 4001 modularity: batch: 0.38143933 full: 0.38143930 best: 0.38143933 time: 0.0025s\n",
            "Epoch: 4501 modularity: batch: 0.38143942 full: 0.38143942 best: 0.38143951 time: 0.0026s\n",
            "Epoch: 5001 modularity: batch: 0.38143948 full: 0.38143948 best: 0.38143951 time: 0.0056s\n",
            "Epoch: 5501 modularity: batch: 0.38143945 full: 0.38143948 best: 0.38143951 time: 0.0024s\n",
            "Epoch: 6001 modularity: batch: 0.38143951 full: 0.38143948 best: 0.38143951 time: 0.0025s\n",
            "Epoch: 6501 modularity: batch: 0.38143951 full: 0.38143951 best: 0.38143954 time: 0.0025s\n",
            "Epoch: 7001 modularity: batch: 0.38143960 full: 0.38143963 best: 0.38143963 time: 0.0025s\n",
            "Epoch: 7501 modularity: batch: 0.38143960 full: 0.38143960 best: 0.38143963 time: 0.0024s\n",
            "Epoch: 8001 modularity: batch: 0.38143960 full: 0.38143963 best: 0.38143963 time: 0.0025s\n",
            "Epoch: 8501 modularity: batch: 0.38143957 full: 0.38143957 best: 0.38143963 time: 0.0024s\n",
            "Epoch: 9001 modularity: batch: 0.38143963 full: 0.38143963 best: 0.38143963 time: 0.0025s\n",
            "Epoch: 9501 modularity: batch: 0.38143957 full: 0.38143957 best: 0.38143963 time: 0.0024s\n",
            "Epoch: 10000 modularity: batch: 0.38143960 full: 0.38143960 best: 0.38143963 time: 0.0035s\n",
            "Optimization Finished with modularity  0.3814396262168884\n",
            "Total time elapsed: 26.9539s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "partitioner = VNNpartitioner(A, X_, out_features = n_comms, hid_layers = 4, hidden_dim = 2 * N, dropout = 0.2, SEED = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt2TtfuCRyvE",
        "outputId": "f8883b47-f467-478d-ebea-30ed8a15c1ce"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "qke0swtBG1Id",
        "outputId": "3fb4a634-d903-4a2c-f4c8-e1440d753ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 batch MSE: 0.14060409 full MSE: 0.13899609 best MSE: 0.13899609 time: 0.0098s\n",
            "Epoch: 0151 batch MSE: 0.13418409 full MSE: 0.12930012 best MSE: 0.12588826 time: 0.0026s\n",
            "Epoch: 0301 batch MSE: 0.10195141 full MSE: 0.09144533 best MSE: 0.08998152 time: 0.0028s\n",
            "Epoch: 0451 batch MSE: 0.05528569 full MSE: 0.05914411 best MSE: 0.05264277 time: 0.0025s\n",
            "Epoch: 0601 batch MSE: 0.02733142 full MSE: 0.04781006 best MSE: 0.04395728 time: 0.0026s\n",
            "Epoch: 0751 batch MSE: 0.02459649 full MSE: 0.03292270 best MSE: 0.03089871 time: 0.0025s\n",
            "Epoch: 0901 batch MSE: 0.00765562 full MSE: 0.01826604 best MSE: 0.01383921 time: 0.0036s\n",
            "Epoch: 1051 batch MSE: 0.01781948 full MSE: 0.01766050 best MSE: 0.00899935 time: 0.0026s\n",
            "Epoch: 1201 batch MSE: 0.01918280 full MSE: 0.01404206 best MSE: 0.00608805 time: 0.0026s\n",
            "Epoch: 1351 batch MSE: 0.00666399 full MSE: 0.00711895 best MSE: 0.00238308 time: 0.0026s\n",
            "Epoch: 1501 batch MSE: 0.02195257 full MSE: 0.00751295 best MSE: 0.00210181 time: 0.0042s\n",
            "Epoch: 1651 batch MSE: 0.01723512 full MSE: 0.01116622 best MSE: 0.00109379 time: 0.0025s\n",
            "Epoch: 1801 batch MSE: 0.00503030 full MSE: 0.00566741 best MSE: 0.00090025 time: 0.0026s\n",
            "Epoch: 1951 batch MSE: 0.00951949 full MSE: 0.00636933 best MSE: 0.00089840 time: 0.0025s\n",
            "Epoch: 2101 batch MSE: 0.00385568 full MSE: 0.00656641 best MSE: 0.00087175 time: 0.0028s\n",
            "Epoch: 2251 batch MSE: 0.00122981 full MSE: 0.00314717 best MSE: 0.00057567 time: 0.0026s\n",
            "Epoch: 2401 batch MSE: 0.00105111 full MSE: 0.00809819 best MSE: 0.00049516 time: 0.0026s\n",
            "Epoch: 2551 batch MSE: 0.00035348 full MSE: 0.00148586 best MSE: 0.00038760 time: 0.0026s\n",
            "Epoch: 2701 batch MSE: 0.00784708 full MSE: 0.01301461 best MSE: 0.00035051 time: 0.0026s\n",
            "Epoch: 2851 batch MSE: 0.00338551 full MSE: 0.00406282 best MSE: 0.00018242 time: 0.0026s\n",
            "Epoch: 3000 batch MSE: 0.00015118 full MSE: 0.00126946 best MSE: 0.00018242 time: 0.0027s\n",
            "Optimization Finished with MSE  0.00018241879297420382\n",
            "Total time elapsed: 8.4756s\n"
          ]
        }
      ],
      "source": [
        "partitioner.fitSupervised(partition, n_epochs = 3000, lr = 0.0005, SEED = 1, batchsize = 20); #pre-tune the model using a supervised fit to the previously acheived clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "NurfidAtG1Id",
        "outputId": "0c2ca555-4bb7-4079-921e-2b2772f05403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 modularity: batch: 0.55041718 full: 0.54953468 best: 0.54953468 time: 0.0076s\n",
            "Epoch: 1001 modularity: batch: 0.56613362 full: 0.56613380 best: 0.56613380 time: 0.0034s\n",
            "Epoch: 2001 modularity: batch: 0.56623113 full: 0.56623125 best: 0.56623125 time: 0.0045s\n",
            "Epoch: 3001 modularity: batch: 0.56626582 full: 0.56626582 best: 0.56626582 time: 0.0025s\n",
            "Epoch: 4001 modularity: batch: 0.56628126 full: 0.56628120 best: 0.56628126 time: 0.0025s\n",
            "Epoch: 5001 modularity: batch: 0.56628889 full: 0.56628889 best: 0.56628889 time: 0.0027s\n",
            "Epoch: 6001 modularity: batch: 0.56629312 full: 0.56629312 best: 0.56629312 time: 0.0026s\n",
            "Epoch: 7001 modularity: batch: 0.56629533 full: 0.56629527 best: 0.56629533 time: 0.0026s\n",
            "Epoch: 8001 modularity: batch: 0.56629658 full: 0.56629652 best: 0.56629658 time: 0.0025s\n",
            "Epoch: 9001 modularity: batch: 0.56629729 full: 0.56629723 best: 0.56629729 time: 0.0025s\n",
            "Epoch: 10001 modularity: batch: 0.56629765 full: 0.56629765 best: 0.56629771 time: 0.0027s\n",
            "Epoch: 11001 modularity: batch: 0.56629789 full: 0.56629795 best: 0.56629795 time: 0.0025s\n",
            "Epoch: 12001 modularity: batch: 0.56629807 full: 0.56629813 best: 0.56629813 time: 0.0025s\n",
            "Epoch: 13001 modularity: batch: 0.56629813 full: 0.56629807 best: 0.56629813 time: 0.0038s\n",
            "Epoch: 14001 modularity: batch: 0.56629819 full: 0.56629819 best: 0.56629819 time: 0.0025s\n",
            "Epoch: 15001 modularity: batch: 0.56629825 full: 0.56629825 best: 0.56629825 time: 0.0025s\n",
            "Epoch: 16001 modularity: batch: 0.56629831 full: 0.56629831 best: 0.56629831 time: 0.0025s\n",
            "Epoch: 17001 modularity: batch: 0.56629831 full: 0.56629831 best: 0.56629831 time: 0.0025s\n",
            "Epoch: 18001 modularity: batch: 0.56629831 full: 0.56629831 best: 0.56629831 time: 0.0026s\n",
            "Epoch: 19001 modularity: batch: 0.56629831 full: 0.56629831 best: 0.56629831 time: 0.0033s\n",
            "Epoch: 20000 modularity: batch: 0.56629831 full: 0.56629831 best: 0.56629831 time: 0.0043s\n",
            "Optimization Finished with modularity  0.5662983059883118\n",
            "Total time elapsed: 54.7014s\n"
          ]
        }
      ],
      "source": [
        "partitioner.dropout = 0.0; #remove dropout for unsupervised fit\n",
        "partitioner.fitUnsupervised(n_epochs = 20000, lr = 0.0005, SEED = 2, batchsize = 20, restore_best = 500); #unsupervised node embedding clustering optimizing modularity "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_cwy8CyFUGQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Embedding_NN_NetClustering.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "aa8c9e988bba47ec3e791b22c0bbb49eb66a938b6cde8fffd564472f09fd563a"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}