{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L73c8k8DtR2w",
    "outputId": "46a4befd-1f4f-49e3-aa7e-8d0facba67ed"
   },
   "outputs": [],
   "source": [
    "#!pip install pycombo\n",
    "#!pip install fastnode2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pycombo\n",
    "#import fastnode2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aH23YvKuTW9y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanislav/opt/anaconda3/envs/Python37pytorch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetworkMat(filename, path = '/Users/stanislav/Desktop/NYU/NYURESEARCH/STRATEGIC_RESEARCH/ModularityMaximum/SampleNetworks/ProcessedMat/'):\n",
    "    A = scipy.io.loadmat(path + filename)\n",
    "    if check_symmetric(A['net']):\n",
    "        G = nx.from_numpy_matrix(A['net'])\n",
    "    else:\n",
    "        G = nx.from_numpy_matrix(A['net'], create_using=nx.DiGraph)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S7bPMdVUTkPm"
   },
   "outputs": [],
   "source": [
    "def modularity_matrix(adj):\n",
    "    w_in = adj.sum(dim=0, keepdim=True)\n",
    "    w_out = adj.sum(dim=1, keepdim=True)\n",
    "    T = w_out.sum()\n",
    "    Q = adj / T - w_out * w_in / T ** 2\n",
    "    return Q\n",
    "\n",
    "def modularity(Q, partition):\n",
    "    return (Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()\n",
    "\n",
    "def residential_page_rank_embedding(A, alpha=0.85):\n",
    "    w = A.sum(axis = 1).reshape(-1,1)\n",
    "    n = A.shape[0]\n",
    "    A = (A + (w == 0)) / (w + n * (w == 0))\n",
    "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
    "    X = (1 - alpha) * AI\n",
    "    return X\n",
    "\n",
    "def node2vec_embedding(G: nx.Graph, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
    "    if nx.is_weighted(G):\n",
    "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1]), edge[2]['weight']) for edge in G.edges(data=True)],\n",
    "                directed=False, weighted=True)\n",
    "    else:\n",
    "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1])) for edge in G.edges(data=True)],\n",
    "                    directed=False, weighted=False)\n",
    "    n2v = fastnode2vec.Node2Vec(n2v_graph, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
    "    n2v.train(epochs=100)\n",
    "    n2v_embeddings = np.array([n2v.wv[str(node)] for node in G])\n",
    "    return n2v_embeddings\n",
    "\n",
    "def rpr_clustering(A: np.array, n_clusters=4, kmeans_runs=100, alpha=0.85, seed=42):\n",
    "    rpr_embedding = residential_page_rank_embedding(A, alpha)\n",
    "    X = StandardScaler().fit_transform(X=rpr_embedding)\n",
    "    rpr_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(X).labels_\n",
    "    return rpr_cluster_labels\n",
    "\n",
    "def n2v_clustering(G: nx.Graph, n_clusters=4, kmeans_runs=100, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
    "    n2v_embeddings = node2vec_embedding(G, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
    "    X = StandardScaler().fit_transform(X=n2v_embeddings)\n",
    "    n2v_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(X).labels_\n",
    "    return n2v_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W15lSK_eM0pN"
   },
   "outputs": [],
   "source": [
    "def aggregateEmbedding(X, n_groups):\n",
    "  alpha = 0.85\n",
    "  kmeans_runs = 100\n",
    "  seed = 1\n",
    "  XS = StandardScaler().fit_transform(X = X)\n",
    "  model = KMeans(n_clusters = n_groups, n_init=kmeans_runs, random_state=seed).fit(XS)\n",
    "  #model = GaussianMixture(n_components = n_groups, n_init=kmeans_runs, random_state=seed).fit(X)\n",
    "  #rpr_cluster_labels = model.predict_proba(X)\n",
    "  cluster_labels = model.predict(XS)\n",
    "  XC = model.cluster_centers_\n",
    "  dist2 = np.array([[((XS[i, :] - XC[j, :])**2).sum() for j in range(n_groups)] for i in range(X.shape[0])])\n",
    "  sigma2 = dist2.min(axis = 1).mean()\n",
    "  probs = np.exp(- dist2 / sigma2)\n",
    "  probs /= probs.sum(axis = 1).reshape(-1,1)\n",
    "  XA = np.matmul(X, probs)\n",
    "  return XA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nDvODpJ0E8I7"
   },
   "outputs": [],
   "source": [
    "def modularity_matrix_np(adj):\n",
    "    if not(isinstance(adj, np.ndarray)):\n",
    "        adj = nx.to_numpy_array(G)\n",
    "    w_in = adj.sum(axis=0).reshape(-1,1)\n",
    "    w_out = adj.sum(axis=1).reshape(1,-1)\n",
    "    T = w_out.sum()\n",
    "    Q = adj / T - w_out * w_in / T ** 2\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterEmbeddings(X, n_comms = 2, cluststyle = 'k-means', seed = 1, runs = 100):\n",
    "    if cluststyle == 'GM':\n",
    "        XS = StandardScaler().fit_transform(X = X)\n",
    "        model = GaussianMixture(n_components = n_comms, n_init=runs, random_state=seed).fit(XS)\n",
    "        partition = model.predict(XS)\n",
    "    else:\n",
    "        XS = StandardScaler().fit_transform(X = X)\n",
    "        model = KMeans(n_clusters=n_comms, n_init=runs, random_state=seed).fit(XS)\n",
    "        partition = model.labels_\n",
    "    return partition, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceAggEmbed(A, n_agg, alpha = 0.85):\n",
    "    X = residential_page_rank_embedding(A, alpha = alpha)\n",
    "    X_ = np.zeros((X.shape[0],0))\n",
    "    for na in n_agg:\n",
    "        if na == 0:\n",
    "            XA = X.copy()\n",
    "        else:\n",
    "            XA = aggregateEmbedding(X, n_groups = na)\n",
    "        X_ = np.concatenate((X_, XA), axis = 1)\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0IH9HxBD8nj9"
   },
   "outputs": [],
   "source": [
    "#Gs =  [(nx.from_numpy_array(nx.to_numpy_array(nx.karate_club_graph(), weight=None)), 'karate', 4)]#, (nx.les_miserables_graph(),'lesmis',6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processNet('karate_34') #karate_34 & 0.419790 & 0.419790 & 0.419790 & 0.419790 & 0.05 & 0.12 & 0.14 & 1.25\n",
    "#processNet('dolphins_62') #ok dolphins_62 & 0.527728 & 0.526463 & 0.527728 & 0.528519 & 0.11 & 0.15 & 0.10 & 1.13\n",
    "#processNet('football_115') #ok football_115 & 0.605445 & 0.605445 & 0.605445 & 0.605445 & 0.22 & 0.26 & 0.34 & 2.89\n",
    "#        processNet('polbooks_105') #ok polbooks_105 & 0.526967 & 0.527237 & 0.527237 & 0.527237 & 0.22 & 0.23 & 0.18 & 1.57\n",
    "#        processNet('copperfield_112') #ok ccopperfield_112 & 0.303028 & 0.309556 & 0.310173 & 0.313301 & 0.29 & 0.33 & 0.22 & 1.99\n",
    "#        processNet('email_1133')  # ok - undirected, unweighted email_1133 & 0.573062 & 0.582755 & 0.572484 & 0.578221 & 5.15 & 47.57 & 7.36 & 71.62\n",
    "#        processNet('lesmis_77') #ok lesmis_77 & 0.566688 & 0.566688 & 0.566688 & 0.566688 & 0.12 & 0.16 & 0.24 & 1.94\n",
    "#        processNet('celegansneural_297')  # directed celegansneural_297 & 0.503509 & 0.507642 & 0.505710 & 0.507642 & 16.43 & 1.19 & 1.46 & 8.61 | seed = 1: & 0.503509 & 0.507605 & 0.506235 & 0.507642 & 16.32 & 1.09 & 1.45 & 8.26\n",
    "#        processNet('jazz_198')  # celegansmetabolic_198 & 0.445627 & 0.444787 & 0.445522 & 0.445627 & 6.28 & 0.35 & 0.83 & 5.50\n",
    "#        processNet('USAir97_332')  # seed = 1; USAir97_332 & 0.207925 & 0.215244 & 0.217149 & 0.217751 & 20.27 & 1.08 & 1.72 & 9.29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66GlS2NRN56_",
    "outputId": "9d9e4798-b615-46f2-e825-1c4e042ec2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network  lesmis_77 ; communities: 10\n"
     ]
    }
   ],
   "source": [
    "#G_ = Gs[0]\n",
    "#G = G_[0]\n",
    "#n_comms = G_[2]\n",
    "\n",
    "nname = 'lesmis_77' \n",
    "#nname = 'copperfield_112'\n",
    "#nname = 'polbooks_105'\n",
    "#nname = 'jazz_198' #0.443\n",
    "G = loadNetworkMat(nname)\n",
    "n_comms = 10\n",
    "\n",
    "print(\"Network \", nname, '; communities:', n_comms)\n",
    "#G = G_[0]\n",
    "A = nx.to_numpy_array(G)\n",
    "Q = modularity_matrix_np(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-0g0ZlEAsad",
    "outputId": "25abdd74-fc35-487e-f04c-e8a04081f8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.541158536585366"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partition aggregated embeddings\n",
    "#cluststyle = 'GM' \n",
    "cluststyle = 'k-means'\n",
    "seed = 12\n",
    "X_ = produceAggEmbed(A, n_agg = [20, 8, 4], alpha = 0.85)\n",
    "N = X_.shape[1]\n",
    "partition, _ = clusterEmbeddings(X = X_, n_comms = n_comms, cluststyle = cluststyle, seed = seed)\n",
    "Q = modularity_matrix_np(A)\n",
    "(Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()\n",
    "#modularity(modularity_matrix(A), np.array(rpr_cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fwmyXMYvTcHf"
   },
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.0):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.weight1 = nn.Parameter(torch.randn(in_features, out_features)) # 0.5 * torch.eye(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(1, out_features)) # -0.5 * torch.ones(1, out_features))\n",
    "        self.dropout = dropout\n",
    "        #xavier initialization\n",
    "        torch.nn.init.xavier_uniform(self.weight1)\n",
    "        torch.nn.init.xavier_uniform(self.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        v1 = torch.mm(input, self.weight1)\n",
    "        output = v1 + self.bias\n",
    "        output = F.dropout(output, p=self.dropout, training=self.training)\n",
    "        return output\n",
    "\n",
    "class GNN_MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hid_layers = 0, hidden_dim=8, dropout=0.0):\n",
    "        #super(GNN_MLP, self).__init__()\n",
    "        super().__init__()\n",
    "        self.n_layers = hid_layers + 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if self.n_layers > 1:\n",
    "            layers = [GNNLayer(in_features, self.hidden_dim, dropout)]\n",
    "        else:\n",
    "            layers = [GNNLayer(in_features, out_features, dropout)]\n",
    "        for _ in range(self.n_layers-2):\n",
    "            layers.append(GNNLayer(self.hidden_dim, self.hidden_dim, dropout))\n",
    "        #if self.n_layers > 1:\n",
    "        layers.append(GNNLayer(self.hidden_dim, out_features, dropout = 0))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = nn.ReLU()(x)\n",
    "        x = self.layers[-1](x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        #x = 1.0 + x - x.max(dim=-1, keepdim=True).values\n",
    "        #x = torch.clamp(x, 0, 1)\n",
    "        #x = x / x.sum(dim=-1, keepdim=True) #normalize st sum = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNNpartitioner (GNN_MLP):\n",
    "    def __init__(self, A, X, out_features = 2, hid_layers = 1, hidden_dim = 12, dropout = 0.0):\n",
    "        self.adj = torch.FloatTensor(A)\n",
    "        self.Q = modularity_matrix(self.adj)\n",
    "        self.features = torch.FloatTensor(X)\n",
    "        super().__init__(self.features.shape[1], out_features = out_features, hid_layers = hid_layers, hidden_dim = hidden_dim, dropout = dropout)\n",
    "         \n",
    "    def fitSupervised(self, target_comms, n_epochs = 1000, lr = 0.005, SEED = 1, batchsize = 0):\n",
    "        track_best = True\n",
    "        C0 = torch.tensor(to_categorical(target_comms))\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        t_total = time.time()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(n_epochs):\n",
    "            batchind = range(self.features.shape[0])\n",
    "            if batchsize > 0:\n",
    "                batchind = np.random.choice(batchind, batchsize)\n",
    "            t_1run = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            out_embed = self.forward(self.features[batchind, :])\n",
    "            C = out_embed[:, :n_comms]\n",
    "            loss = torch.mean(torch.square(torch.subtract(C,C0[batchind,:]))) #torch.nn.functional.binary_cross_entropy(C, C0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if track_best:\n",
    "                if batchsize > 0:\n",
    "                    full_embed = self.forward(self.features)\n",
    "                    C = full_embed[:, :n_comms]\n",
    "                    full_loss = torch.mean(torch.square(torch.subtract(C,C0)))\n",
    "                else:\n",
    "                    full_loss = loss\n",
    "            \n",
    "                if epoch == 0 or full_loss < best_loss:\n",
    "                    best_loss = full_loss #- torch.trace(Q)\n",
    "                    best_C = C.data\n",
    "                    best_embed = out_embed.data\n",
    "                    best_epoch = epoch\n",
    "                \n",
    "            else:\n",
    "                full_loss = loss\n",
    "                best_loss = loss\n",
    "                \n",
    "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
    "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                print('Epoch: {:04d}'.format(epoch + 1), 'batch MSE: {:.8f}'.format(loss.item()), 'full MSE: {:.8f}'.format(full_loss.item()),\n",
    "                        'best MSE: {:.8f}'.format(best_loss.item()),\n",
    "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
    "        ent = best_loss.item()\n",
    "        print(\"Optimization Finished with MSE \", ent)\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        return C, ent\n",
    "    \n",
    "    def fitUnsupervised(self, n_epochs = 1000, lr = 0.005, SEED = 1, batchsize = 0, restore_best = 500):\n",
    "        track_best = True\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        t_total = time.time()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        out_embed = self.forward(self.features)\n",
    "        C = out_embed[:, :n_comms].data\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            t_1run = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            batchind = range(self.features.shape[0])\n",
    "            if batchsize > 0:\n",
    "                batchind = np.random.choice(batchind, batchsize)\n",
    "            out_embed = self.forward(self.features[batchind, :])\n",
    "            C[batchind, :] = out_embed[:, :n_comms]\n",
    "            Q1 = torch.mm(C.T, self.Q)\n",
    "            Q2 = torch.mm(Q1, C)\n",
    "            loss = torch.trace(Q2)\n",
    "            loss = -loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            C = C.data\n",
    "            \n",
    "            if track_best:\n",
    "                if batchsize > 0:\n",
    "                    full_embed = self.forward(self.features)\n",
    "                    C = full_embed[:, :n_comms].data\n",
    "                    full_loss = - torch.trace(torch.mm(torch.mm(C.T, self.Q), C))\n",
    "                else:\n",
    "                    full_loss = loss\n",
    "            \n",
    "                if epoch == 0 or full_loss < best_loss:\n",
    "                    best_loss = full_loss #- torch.trace(Q)\n",
    "                    best_C = C.data\n",
    "                    best_embed = out_embed.data\n",
    "                    best_epoch = epoch\n",
    "                    \n",
    "                    torch.save(self.state_dict(), 'model_scripted.pt')\n",
    "                    \n",
    "                if (epoch + 1) % restore_best == 0:\n",
    "                    self.load_state_dict(torch.load('model_scripted.pt'))\n",
    "                    self.eval()\n",
    "                \n",
    "            else:\n",
    "                full_loss = loss\n",
    "                best_loss = loss\n",
    "            \n",
    "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
    "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                print('Epoch: {:04d}'.format(epoch + 1), 'modularity: batch: {:.8f}'.format(-loss.item()), 'full: {:.8f}'.format(-full_loss.item()),\n",
    "                        'best: {:.8f}'.format(-best_loss.item()),\n",
    "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
    "                #print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                #        'Modularity: {:.8f}'.format(-best_loss.item()),\n",
    "                #        'time: {:.4f}s'.format(time.time() - t_1run))\n",
    "        mod = -best_loss.item()\n",
    "        print(\"Optimization Finished with modularity \", mod)\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        return C, mod\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combo_comms, combo_mod = pycombo.execute(G, max_communities = 3); combo_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = produceAggEmbed(A, n_agg = [20, 8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#augment X_ with cluster centroids\n",
    "#partition, model = clusterEmbeddings(X = X_, n_comms = n_comms)\n",
    "#XC = model.cluster_centers_\n",
    "#XC = np.array((list(XC.flatten()) * X_.shape[0])).reshape(X_.shape[0],-1)\n",
    "#X_ = np.concatenate((X_, XC), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanislav/opt/anaconda3/envs/Python37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/Users/stanislav/opt/anaconda3/envs/Python37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "partitioner = VNNpartitioner(A, X_, out_features = n_comms, hid_layers = 4, hidden_dim = 2*N, dropout = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 batch MSE: 0.09287868 full MSE: 0.09311878 best MSE: 0.09311878 time: 0.0029s\n",
      "Epoch: 0251 batch MSE: 0.04260942 full MSE: 0.04416693 best MSE: 0.04307650 time: 0.0022s\n",
      "Epoch: 0501 batch MSE: 0.00362893 full MSE: 0.01388867 best MSE: 0.01326894 time: 0.0024s\n",
      "Epoch: 0751 batch MSE: 0.01411554 full MSE: 0.01105171 best MSE: 0.00828037 time: 0.0022s\n",
      "Epoch: 1001 batch MSE: 0.00530259 full MSE: 0.01071591 best MSE: 0.00681864 time: 0.0022s\n",
      "Epoch: 1251 batch MSE: 0.00700580 full MSE: 0.00808174 best MSE: 0.00563335 time: 0.0025s\n",
      "Epoch: 1501 batch MSE: 0.00798272 full MSE: 0.00623938 best MSE: 0.00534532 time: 0.0023s\n",
      "Epoch: 1751 batch MSE: 0.00109555 full MSE: 0.00323535 best MSE: 0.00240318 time: 0.0021s\n",
      "Epoch: 2001 batch MSE: 0.01078378 full MSE: 0.00202744 best MSE: 0.00127370 time: 0.0021s\n",
      "Epoch: 2251 batch MSE: 0.00003505 full MSE: 0.00192861 best MSE: 0.00034608 time: 0.0030s\n",
      "Epoch: 2501 batch MSE: 0.00529000 full MSE: 0.00082043 best MSE: 0.00016680 time: 0.0026s\n",
      "Epoch: 2751 batch MSE: 0.00134656 full MSE: 0.00012681 best MSE: 0.00009641 time: 0.0022s\n",
      "Epoch: 3001 batch MSE: 0.00028224 full MSE: 0.00031435 best MSE: 0.00004438 time: 0.0024s\n",
      "Epoch: 3251 batch MSE: 0.00009087 full MSE: 0.00038767 best MSE: 0.00004438 time: 0.0022s\n",
      "Epoch: 3501 batch MSE: 0.00006384 full MSE: 0.00012211 best MSE: 0.00001611 time: 0.0028s\n",
      "Epoch: 3751 batch MSE: 0.00000531 full MSE: 0.00005324 best MSE: 0.00001136 time: 0.0028s\n",
      "Epoch: 4001 batch MSE: 0.00001245 full MSE: 0.00014593 best MSE: 0.00000504 time: 0.0022s\n",
      "Epoch: 4251 batch MSE: 0.00013228 full MSE: 0.00005689 best MSE: 0.00000504 time: 0.0022s\n",
      "Epoch: 4501 batch MSE: 0.00000099 full MSE: 0.00009468 best MSE: 0.00000504 time: 0.0026s\n",
      "Epoch: 4751 batch MSE: 0.00031297 full MSE: 0.00021643 best MSE: 0.00000469 time: 0.0023s\n",
      "Epoch: 5000 batch MSE: 0.00000025 full MSE: 0.00013379 best MSE: 0.00000425 time: 0.0023s\n",
      "Optimization Finished with MSE  4.251784048392437e-06\n",
      "Total time elapsed: 12.5308s\n"
     ]
    }
   ],
   "source": [
    "partitioner.fitSupervised(partition, n_epochs = 5000, lr = 0.0005, SEED = 2, batchsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner.dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 modularity: batch: 0.54844773 full: 0.54844767 best: 0.54844767 time: 0.0036s\n",
      "Epoch: 0501 modularity: batch: 0.54844737 full: 0.54844707 best: 0.54844773 time: 0.0018s\n",
      "Epoch: 1001 modularity: batch: 0.54844773 full: 0.54844761 best: 0.54844773 time: 0.0018s\n",
      "Epoch: 1501 modularity: batch: 0.56502473 full: 0.56502473 best: 0.56502473 time: 0.0017s\n",
      "Epoch: 2001 modularity: batch: 0.56503075 full: 0.56503075 best: 0.56503075 time: 0.0017s\n",
      "Epoch: 2501 modularity: batch: 0.56503189 full: 0.56503189 best: 0.56503189 time: 0.0018s\n",
      "Epoch: 3001 modularity: batch: 0.56503248 full: 0.56503248 best: 0.56503248 time: 0.0017s\n",
      "Epoch: 3501 modularity: batch: 0.56503278 full: 0.56503278 best: 0.56503278 time: 0.0018s\n",
      "Epoch: 4001 modularity: batch: 0.56503302 full: 0.56503302 best: 0.56503302 time: 0.0018s\n",
      "Epoch: 4501 modularity: batch: 0.56503308 full: 0.56503308 best: 0.56503308 time: 0.0018s\n",
      "Epoch: 5001 modularity: batch: 0.56503314 full: 0.56503308 best: 0.56503314 time: 0.0022s\n",
      "Epoch: 5501 modularity: batch: 0.56503320 full: 0.56503320 best: 0.56503320 time: 0.0018s\n",
      "Epoch: 6001 modularity: batch: 0.56503320 full: 0.56503320 best: 0.56503320 time: 0.0021s\n",
      "Epoch: 6501 modularity: batch: 0.56503320 full: 0.56503320 best: 0.56503326 time: 0.0018s\n",
      "Epoch: 7001 modularity: batch: 0.56503326 full: 0.56503326 best: 0.56503326 time: 0.0017s\n",
      "Epoch: 7501 modularity: batch: 0.56503326 full: 0.56503326 best: 0.56503326 time: 0.0018s\n",
      "Epoch: 8001 modularity: batch: 0.56503326 full: 0.56503320 best: 0.56503326 time: 0.0018s\n",
      "Epoch: 8501 modularity: batch: 0.56503332 full: 0.56503332 best: 0.56503332 time: 0.0018s\n",
      "Epoch: 9001 modularity: batch: 0.56503338 full: 0.56503338 best: 0.56503338 time: 0.0018s\n",
      "Epoch: 9501 modularity: batch: 0.56503332 full: 0.56503332 best: 0.56503338 time: 0.0018s\n",
      "Epoch: 10000 modularity: batch: 0.56503338 full: 0.56503338 best: 0.56503338 time: 0.0024s\n",
      "Optimization Finished with modularity  0.5650333762168884\n",
      "Total time elapsed: 18.6541s\n"
     ]
    }
   ],
   "source": [
    "partitioner.fitUnsupervised(n_epochs = 10000, lr = 0.0001, SEED = 1, batchsize = 20, restore_best = 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Embedding_NN_HierClustering.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aa8c9e988bba47ec3e791b22c0bbb49eb66a938b6cde8fffd564472f09fd563a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
